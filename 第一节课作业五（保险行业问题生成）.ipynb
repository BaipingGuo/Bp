{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业：实践编程部分\n",
    "\n",
    "情境：保险行业对话的LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理：去除分隔符和标点符号，将txt文本转成列表，取列表中的中文、英文，分别生成一个一维列表以储存全部string类型的中英文数据\n",
    "\n",
    "corpus='C:/Users/GuoBaiPing/Desktop/个人文件/人工智能与NLP 5期/train.txt' #注意/\n",
    "FILE=open(corpus,'r',encoding='UTF-8') #encoding='UTF-8'写入，防止python读取出错\n",
    "lines = FILE.readlines() #readlines()函数会把txt文本每一行的回车符\\n读取出来\n",
    "sub_file_cn=[] #将目标文件封装进一个空列表\n",
    "sub_file_en=[]\n",
    "\n",
    "for line in lines:\n",
    "    items = line.strip().split(' ++$++ ') #line.strp()旨在将\\n去除。本段循环可以封装到一个def函数里\n",
    "    sub_file_cn.append(items[2]) #items将每一行分成若干列\n",
    "    sub_file_cn=''.join(sub_file_cn).split('？')#将sub_file_cn看做一个sequence，去除标点符号后缀合成语料\n",
    "    sub_file_en.append(items[3])\n",
    "    sub_file_en=' '.join(sub_file_en).split('?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\GUOBAI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.875 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003174603174603174"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "import jieba\n",
    "\n",
    "cn_tokens=list(jieba.cut(sub_file_cn[0])) #中文切词\n",
    "words_count=Counter(cn_tokens) #1-gram计数，生成列表\n",
    "\n",
    "_2_gram_words = [cn_tokens [i] + cn_tokens [i+1] for i in range(len(cn_tokens)-1)] #生成2-gram的tokens\n",
    "_2_gram_word_counts = Counter(_2_gram_words) #2-gram计数，生成列表\n",
    "\n",
    "def get_1_gram_count(word):\n",
    "    if word in words_count : \n",
    "        return words_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1] #确保如果找不到该词，必定返回最小的计数，即1，这样在进行除法运算时不会出现分子为0的情况\n",
    "\n",
    "\n",
    "def get_2_gram_count(word):\n",
    "    if word in _2_gram_words : \n",
    "        return _2_gram_word_counts[word]\n",
    "    else:\n",
    "        return _2_gram_word_counts.most_common()[-1][-1] \n",
    "    \n",
    "def _2_gram_models(sentence):\n",
    "    tokens=list(jieba.cut(sentence)) #将input的语句进行切分\n",
    "    probability=1 #因为是个似然函数，所以初始化为1\n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word=tokens[i+1]\n",
    "        pro=get_2_gram_count(word+next_word) / get_1_gram_count(word)\n",
    "        probability *= pro #似然计算\n",
    "    return probability\n",
    "\n",
    "_2_gram_models('怎么提交人寿保险')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "合理提问 = \"\"\"\n",
    "提问 = 代词 动作 险种 | 情境 判断 动作 险种\n",
    "代词 = 怎样 | 怎么 | 如何 \n",
    "动作 = 提交 | 覆盖 | 报告 | 接受 | 需要 | 逆转 | 缴交 | 有\n",
    "险种 = 人寿保险 | 车险 | 残疾保险 | 健康保险 | 医疗保险 | 租赁保险\n",
    "情境 = 我的车剐蹭 | 如果已经诊断重疾 | 离婚情况 | 雇主收取吸烟者费用\n",
    "判断 = 是否 | 可不可以 | 能不能 | 行不行\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "#整理用于随机读取的数据库 ---- 使用字典进行存储\n",
    "def get_generation(raw_string, target, stmt_split = '=', or_split = '|'):\n",
    "    rules = dict() #对象为字典\n",
    "    for line in raw_string.split('\\n'): #与预处理的数据特点有关\n",
    "        if not line: continue\n",
    "        stmt, expression = line.split(stmt_split) #分开\n",
    "        rules[stmt.strip()]=expression.split(or_split) # 分开的同时去掉首尾的空格，确保映射\n",
    "        generated = generate (rules, target = target) # 嵌套，见下\n",
    "    return generated\n",
    "\n",
    "def generate(rule, target):\n",
    "    if target in rule: \n",
    "        words = rule[target] #在字典中，取得对应的语法数据集\n",
    "        word = random.choice(words) #随机选择一套语法\n",
    "        return ''.join(generate(rule, target=c.strip()) for c in word.split()) #去掉首尾空格\n",
    "    else:\n",
    "        return target #如果映射不对，则会回到原先的语法规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "怎样报告人寿保险\n"
     ]
    }
   ],
   "source": [
    "queries=()\n",
    "q_list=[]\n",
    "for i in range(8):\n",
    "    question=get_generation(合理提问,target='提问')\n",
    "    pro=_2_gram_models(question)\n",
    "    queries=(question, pro)\n",
    "    q_list.append(queries)\n",
    "\n",
    "sort_result=sorted(q_list, key=lambda x: x[1], reverse=True)\n",
    "print(sort_result[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存在问题：\n",
    "1） 提问的生成依赖文本的预处理，所以提问还是有些奇怪；\n",
    "2） 需要提高概率值，但提高到怎样的概率值才不会出现过拟合现象？\n",
    "3） 代码方面，还得考虑泛化……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
